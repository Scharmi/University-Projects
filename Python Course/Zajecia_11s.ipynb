{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc95cf2b",
   "metadata": {},
   "source": [
    "# Część 1: Podstawy HTML i XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a6603",
   "metadata": {},
   "source": [
    "1. Tagi, atrybuty, i elementy:\n",
    "\n",
    "* Tagi to podstawowe składniki języka HTML i XML. Są one umieszczane w nawiasach ostrokątnych, np. &lt;html>, &lt;body>,  &lt;div>.\n",
    "* Atrybuty dostarczają dodatkowych informacji o tagach. Przykład: w tagu &lt;a href=\"https://example.com\"&gt;, href jest atrybutem definiującym adres URL linku.\n",
    "* Elementy składają się z otwierającego tagu, zawartości i zamykającego tagu. Na przykład, &lt;p>To jest paragraf.&lt;/p>, &lt;a> href=hiperłącze &lt;/a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4be8a",
   "metadata": {},
   "source": [
    "2. Nagłówki w HTML\n",
    "* Nagłówki to elementy HTML używane do organizacji i strukturyzacji treści na stronie internetowej. Są one oznaczane tagami od &lt;h1> do &lt;h6>.\n",
    "* &lt;h1> reprezentuje najważniejszy nagłówek na stronie, zwykle tytuł lub główny punkt strony, a &lt;h6> jest najmniej istotnym nagłówkiem.\n",
    "* Użycie nagłówków pomaga w tworzeniu hierarchii informacji na stronie, co jest ważne zarówno dla użytkowników, jak i dla wyszukiwarek internetowych.\n",
    "Przykład:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d158141",
   "metadata": {},
   "source": [
    "<h1>Tytuł główny strony</h1>\n",
    "<h2>Podsekcja 1</h2>\n",
    "<p>Treść podsekcji 1...</p>\n",
    "<h2>Podsekcja 2</h2>\n",
    "<p>Treść podsekcji 2...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a8333",
   "metadata": {},
   "source": [
    "3. HTML a XML:\n",
    "* HTML jest używany głównie do tworzenia stron internetowych i jest bardziej elastyczny co do składni.\n",
    "XML służy do przechowywania i przesyłania danych i wymaga ścisłego przestrzegania zasad dobrze sformowanego dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab8a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie przykładowego pliku XML\n",
    "xml_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<library>\n",
    "    <book>\n",
    "        <title>Przygody Tomka Sawyera</title>\n",
    "        <author>Mark Twain</author>\n",
    "        <year>1876</year>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>Pan Tadeusz</title>\n",
    "        <author>Adam Mickiewicz</author>\n",
    "        <year>1834</year>\n",
    "    </book>\n",
    "</library>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093e720",
   "metadata": {},
   "source": [
    "4. Narzędzia do inspekcji strony:\n",
    "\n",
    "Narzędzia deweloperskie w przeglądarkach internetowych (takie jak Chrome, Firefox, Edge) pozwalają na oglądanie struktury HTML, stylów CSS i skryptów JavaScript strony. Można je otworzyć klikając prawym przyciskiem myszy na stronie i wybierając \"Zbadaj\" lub naciskając F12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90449daa",
   "metadata": {},
   "source": [
    "**Ćwiczenie:**\n",
    "Znajdowanie i wyświetlanie tytułu oraz nagłówków\n",
    "\n",
    "Należy utworzyć bardzo prostą stronę internetową zawierającą tytuł, kilka nagłówków i paragrafów. Jeden paragraf ma zawierać hiperłącze do strony MiMUW. Utworzoną stronę proszę zapisać w pliku i podejrzeć w przeglądarce z wykorzystaniem \"Zbadaj\" (zazwyczaj F12).\n",
    "\n",
    "Ewentualnie można skorzystać z poniższej przykładowej implementacji (w zależności od znajomości HTML). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeebe715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie przykładowej strony HTML (do późniejszego przetwarzania przy użyciu BeautifulSoup)\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html> \n",
    "<html>\n",
    "<head>\n",
    "    <title>Przykładowa strona - Webscraping</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Nagłówek Poziomu 1</h1>\n",
    "    <h2>Nagłówek Poziomu 2</h2>\n",
    "    <p>To jest paragraf na stronie.</p>\n",
    "    <p>To jest kolejny paragraf z kilkoma <a href=\"https://www.mimuw.edu.pl/\">linkami</a> w treści.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c3904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poniżej rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ewentualnie można wczytać już jakąś istniejącą stronę - w tym celu zapiszemy ją i wczytamy za chwilkę jako string\n",
    "import os\n",
    "# Pobierz ścieżkę aktualnego pliku\n",
    "current_file_path = os.getcwd()\n",
    "# Połącz ścieżkę z 'index.html'\n",
    "html_filename = 'index.html'\n",
    "html_file_path = os.path.join(current_file_path, html_filename)\n",
    "with open(html_file_path, 'w') as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd083725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po zapisie strony możemy otworzyć ją (domyślnie powinna otworzyć się w przeglądarce internetowej) i obejrzeć w jaki sposób \n",
    "# wygląda i widoczna jest w trybie inspektora (F12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c16f8",
   "metadata": {},
   "source": [
    "# Część 2: Parsowanie HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe75b4",
   "metadata": {},
   "source": [
    "W tej części skupimy się na narzędziach wykorzystywanych do parsowania treści HTML. Bardzo popularnym pakietem jest Beautiful Soup. Jest to pakiet, który służy do parsowania dokumentów HTML i XML. Jest szczególnie użyteczny w web scrapingu, czyli procesie ekstrakcji danych z stron internetowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cebc84",
   "metadata": {},
   "source": [
    "Napiszmy więc kod, w którym wykorzystamy pakiet BeautifulSoup do analizy naszej strony i wyszukania w niej nagłówków, tytułów oraz paragrafów. Przekonamy się, jak łatwo można przetwarzać HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7f5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Wczytywanie zawartości strony HTML\n",
    "with open(html_file_path, 'r') as file:\n",
    "    html_page = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d71c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html> \\n<html>\\n<head>\\n    <title>Przykładowa strona - Webscraping</title>\\n</head>\\n<body>\\n    <h1>Nagłówek Poziomu 1</h1>\\n    <h2>Nagłówek Poziomu 2</h2>\\n    <p>To jest paragraf na stronie.</p>\\n    <p>To jest kolejny paragraf z kilkoma <a href=\"https://www.mimuw.edu.pl/\">linkami</a> w treści.</p>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ab001",
   "metadata": {},
   "source": [
    "Utworzymy obiekt BeautifulSoup - jako parser wybieramy wbudowany (nie wymagający instalacji/importu dodatkowych pakietów) parser html.\n",
    "Wybór parsera zależny jest od założeń oraz wymagań projektów, jednakże dla tak małych stron html.parser jest wystarczający.\n",
    "W przypadku znacznie większych stron warto rozważyć wykorzystanie lxml, który jest szybszy (wymaga jednak instalacji dodatkowych zależności). Więcej na ten temat można poczytać w dokumentacji:\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00bdd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace7a1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>Przykładowa strona - Webscraping</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Nagłówek Poziomu 1</h1>\n",
       "<h2>Nagłówek Poziomu 2</h2>\n",
       "<p>To jest paragraf na stronie.</p>\n",
       "<p>To jest kolejny paragraf z kilkoma <a href=\"https://www.mimuw.edu.pl/\">linkami</a> w treści.</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3127db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Przykładowa strona - Webscraping\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Nagłówek Poziomu 1\n",
      "  </h1>\n",
      "  <h2>\n",
      "   Nagłówek Poziomu 2\n",
      "  </h2>\n",
      "  <p>\n",
      "   To jest paragraf na stronie.\n",
      "  </p>\n",
      "  <p>\n",
      "   To jest kolejny paragraf z kilkoma\n",
      "   <a href=\"https://www.mimuw.edu.pl/\">\n",
      "    linkami\n",
      "   </a>\n",
      "   w treści.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I bardziej czytelnie\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05244ba1",
   "metadata": {},
   "source": [
    "A więc mamy już sparsowaną treść naszej strony - możemy więc przejść do wyszukiwania na niej informacji. W naszym przypadku (choć jest ich niewiele) - możemy pobrać tytuł, nagłówki oraz paragrafy. Spróbujemy również pobrać link do strony MiMUW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaf87b",
   "metadata": {},
   "source": [
    "Tytuł strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9a59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie tytułu strony\n",
    "page_title = soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d259de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Przykładowa strona - Webscraping</title>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "360c88e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Przykładowa strona - Webscraping'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef099",
   "metadata": {},
   "source": [
    "Nagłówki - h1, h2. Wykorzystamy metody\n",
    "* find() - służy do wyszukiwania pierwszego wystąpienia danego tagu lub tagów spełniających określone kryteria.\n",
    "* find_all() - znajduje wszystkie tagi spełniające te kryteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560cbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie wszystkich nagłówków h1 i h2\n",
    "headers_h1 = soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b84b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'string'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mheaders_h1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m \u001b[38;5;66;03m# Nie działa - dlaczego? find_all zwraca listę\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bs4/element.py:2428\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2430\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'string'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "headers_h1.string # Nie działa - dlaczego? find_all zwraca listę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5952b641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nagłówek Poziomu 1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_h1[0].string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e903ec",
   "metadata": {},
   "source": [
    "No ale w naszym przypadku mamy przecież 1 element h1 (h2 również), a więc w zupełności wystarczyłoby find. \"W naszym przypadku\", gdyż zazwyczaj nie znamy strony i może ona zawierać setki różnego rodzaju tagów. Bezpieczniej więc skorzystać z find_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dae25258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla porównania - find\n",
    "headers_h2 = soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8869649e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nagłówek Poziomu 2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_h2.string # Tutaj już w porządku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ec50b",
   "metadata": {},
   "source": [
    "Wyszukiwanie wszystkich paragrafów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3844e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie wszystkich paragrafów\n",
    "paragraphs = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b41cedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>To jest paragraf na stronie.</p>,\n",
       " <p>To jest kolejny paragraf z kilkoma <a href=\"https://www.mimuw.edu.pl/\">linkami</a> w treści.</p>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e8733d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To jest paragraf na stronie.', None]\n"
     ]
    }
   ],
   "source": [
    "print([p.string for p in paragraphs]) # Pod indeksem 1 mamy None, dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9742cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs[1].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30273683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>To jest kolejny paragraf z kilkoma <a href=\"https://www.mimuw.edu.pl/\">linkami</a> w treści.</p>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[1] # Ponieważ w paragrafie znajduje się tag a. Ten z kolei ma atrybut href i jakąś przypisną mu wartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f8c169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://www.mimuw.edu.pl/\">linkami</a>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutaj postępujemy w następujący sposób\n",
    "paragraphs[1].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61ba9e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://www.mimuw.edu.pl/\">linkami</a>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub\n",
    "paragraphs[1].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "289640d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.mimuw.edu.pl/\">linkami</a>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub \n",
    "paragraphs[1].find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bd6e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.mimuw.edu.pl/\">linkami</a>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub - możliwości jest bardzo dużo\n",
    "# Select - umożliwia wyszukiwanie elementów za pomocą selektorów CSS.\n",
    "paragraphs[1].select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71c92f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n"
     ]
    }
   ],
   "source": [
    "# A następnie odwołujemy się do atrybutu href i mamy hiperłącze do MiMUW :)\n",
    "print(paragraphs[1].a['href'])\n",
    "print(paragraphs[1].find('a')['href'])\n",
    "print(paragraphs[1].find_all('a')[0]['href'])\n",
    "print(paragraphs[1].select('a')[0]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dc6b7",
   "metadata": {},
   "source": [
    "Zaprezentowanych zostanie teraz kilka innych przydatnych metod z ich wykorzystaniem - na przykładzie naszej strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1a0907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find() - wyszukuje pierwszy paragraf\n",
    "first_paragraph = soup.find('p').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a8be6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all() - znajduje wszystkie paragrafy\n",
    "all_paragraphs = [p.text for p in soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72a43d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select() - wybiera wszystkie linki (tagi a) w dokumencie\n",
    "all_links = soup.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f0aeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_one() - wybiera pierwszy tag h1\n",
    "first_h1 = soup.select_one('h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89c0a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_parent() - znajduje rodzica pierwszego linku (w tym przypadku paragraf)\n",
    "parent_of_first_link = soup.find('a').find_parent().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbb35405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_next_sibling() - znajduje następujące rodzeństwo po pierwszym nagłówku h1 (w tym przypadku h2)\n",
    "next_sibling_of_h1 = soup.find('h1').find_next_sibling().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec3cbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atrybuty - pobiera atrybut href pierwszego linku\n",
    "first_link_href = soup.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28d615f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyniki\n",
    "results = {\n",
    "    \"Pierwszy paragraf\": first_paragraph,\n",
    "    \"Wszystkie paragrafy\": all_paragraphs,\n",
    "    \"Wszystkie linki\": [link['href'] for link in all_links],\n",
    "    \"Pierwszy nagłówek h1\": first_h1,\n",
    "    \"Rodzic pierwszego linku\": parent_of_first_link,\n",
    "    \"Następne rodzeństwo po h1\": next_sibling_of_h1,\n",
    "    \"Href pierwszego linku\": first_link_href\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de752611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pierwszy paragraf': 'To jest paragraf na stronie.',\n",
       " 'Wszystkie paragrafy': ['To jest paragraf na stronie.',\n",
       "  'To jest kolejny paragraf z kilkoma linkami w treści.'],\n",
       " 'Wszystkie linki': ['https://www.mimuw.edu.pl/'],\n",
       " 'Pierwszy nagłówek h1': 'Nagłówek Poziomu 1',\n",
       " 'Rodzic pierwszego linku': 'To jest kolejny paragraf z kilkoma linkami w treści.',\n",
       " 'Następne rodzeństwo po h1': 'Nagłówek Poziomu 2',\n",
       " 'Href pierwszego linku': 'https://www.mimuw.edu.pl/'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51abffb",
   "metadata": {},
   "source": [
    "Select vs find\n",
    "\n",
    "* find i find_all skupiają się na atrybutach i nazwach tagów.\n",
    "* select i select_one zapewniają większą elastyczność dzięki wykorzystaniu selektorów CSS, co pozwala na bardziej złożone zapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91accf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = \"\"\"\n",
    "<div>\n",
    "    <p class=\"text\">Pierwszy paragraf</p>\n",
    "    <p class=\"text\">Drugi paragraf</p>\n",
    "    <p id=\"special\">Trzeci paragraf</p>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82e4aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92d2b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla tak zdefiniowanej strony mamy:\n",
    "first_paragraph = soup2.find('p')  # Znajduje pierwszy paragraf\n",
    "all_paragraphs = soup2.find_all('p')  # Znajduje wszystkie paragrafy\n",
    "special_paragraph = soup2.select_one('#special')  # Znajduje paragraf z ID 'special'\n",
    "text_paragraphs = soup2.select('p.text')  # Znajduje paragrafy z klasą 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19cbaeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\">Pierwszy paragraf</p>, <p class=\"text\">Drugi paragraf</p>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c90a5d",
   "metadata": {},
   "source": [
    "Aby osiągnąć za pomocą find znalezienie paragrafów z klasy text musimy wykorzystać argument _class:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b8e3d68",
   "metadata": {},
   "source": [
    "text_paragraphs = soup2.find_all('p', class_='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c3dd912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\">Pierwszy paragraf</p>, <p class=\"text\">Drugi paragraf</p>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e154734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\">Pierwszy paragraf</p>, <p class=\"text\">Drugi paragraf</p>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ewentualnie\n",
    "soup2.find_all('p', attrs=[{\"class\": \"text\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6dd19c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"special\">Trzeci paragraf</p>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warto pokazać tutaj również wykorzystanie id (attrs)\n",
    "soup2.find_all('p', {\"id\": \"special\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a5cae",
   "metadata": {},
   "source": [
    "# Część 3: Pakiet requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c25f9",
   "metadata": {},
   "source": [
    "Wiedza z poprzednich części będzie teraz wykorzystana w praktyce. Zanim przejdziemy do ćwiczenia zapoznamy się z pakietem requests, który umożliwi nam wysyłanie żądań HTTP. Obsługuje on wszystkie popularne metody HTTP, takie jak GET, POST, PUT, DELETE...\n",
    "Inne pakiety tego typu to np. wbudowane urllib oraz http.client. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93393f9e",
   "metadata": {},
   "source": [
    "Aby korzystać z requests, najpierw trzeba zainstalować pakiet. Możemy to zrobić za pomocą pip (systemu zarządzania pakietami w Pythonie):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d66a1c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\marcin\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63076652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13651da",
   "metadata": {},
   "source": [
    "POST Request\n",
    "\n",
    "Zapytanie POST służy do wysyłania danych do serwera, na przykład przy przesyłaniu formularza. Niekótre z metod przetestujemy na stronie https://httpbin.org/#/, która umożliwia testowanie zapytań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7349d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.31.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6575b443-46ab727c1e09f9421d188fad\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"77.222.255.248\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "\n",
    "response = requests.post('https://httpbin.org/post', data=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c781b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response # Zwraca kod żądania. Więcej można poczytać na przykład tutaj: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9fa9d",
   "metadata": {},
   "source": [
    "Krótka interpretacja: \n",
    "\n",
    "args: pusty obiekt, co oznacza, że do żądania POST nie zostały dołączone żadne parametry URL\n",
    "\n",
    "data: jest puste, co wskazuje, że żadne dane nie zostały wysłane w ciele żądania w formacie innym niż formularz\n",
    "\n",
    "files: jest puste - w żądaniu POST nie wysłano żadnych plików\n",
    "\n",
    "form: pokazuje dane, które zostały przesłane za pomocą żądania POST. W tym przypadku są to klucze i wartości podane w metodzie post.\n",
    "\n",
    "headers: zawiera nagłówki przesłane razem z żądaniem. Są one automatycznie dodawane przez pakiet requests lub serwer. Poniżej opis kilku z nich.\n",
    "\n",
    "* Host: nazwa hosta, do którego skierowane jest żądanie\n",
    "* User-Agent: identyfikuje klienta wykonującego żądanie, tutaj python-requests/2.31.0 oznacza, że wykorzystano pakiet requests w Pythonie.\n",
    "\n",
    "origin: pokazuje adres IP, z którego wysłano żądanie.\n",
    "\n",
    "url: adres URL, na który wysłano żądanie POST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c8287",
   "metadata": {},
   "source": [
    "GET Request\n",
    "\n",
    "Zapytanie GET służy do pobierania danych z określonego zasobu, a więc jest ono najbardziej przydatne podczas webscrapingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebd34185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://example.com')\n",
    "print(response.text)  # Wyświetla zawartość strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8e4db49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056cced",
   "metadata": {},
   "source": [
    "A więc możemy pobrać zawartość strony (jej treść HTML) z wykorzystaniem requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a500483",
   "metadata": {},
   "source": [
    "**Ćwiczenie:**\n",
    "Wyszukiwanie poszczególnych elementów na stronie\n",
    "\n",
    "W tym zadaniu proszę (wykorzystując Beautiful Soup) wypisać treść nagłówków, paragrafaów oraz zlokalizować hiperłącza znajdujące się na stronie https://example.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eea9f577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eef8e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c4b75e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nagłóweki ': ['Example Domain'],\n",
       " 'Wszystkie paragrafy': ['This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.',\n",
       "  'More information...'],\n",
       " 'Wszystkie linki': ['https://www.iana.org/domains/example']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a330f",
   "metadata": {},
   "source": [
    "Nagłówki\n",
    "\n",
    "Możemy sami definiować nagłówki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ae5a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Marcin\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6575b4c2-616d63f6557ad10d3a183303\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"77.222.255.248\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Marcin'}\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "\n",
    "response = requests.post('https://httpbin.org/post', data=payload, headers=headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541af7eb",
   "metadata": {},
   "source": [
    "Teraz sami zdefiniowaliśmy nagłówek i widoczne jest to w odpowiedzi. Niekiedy działanie takie może być bardzo przydatne:)\n",
    "\n",
    " \"User-Agent\": \"Marcin\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaa444",
   "metadata": {},
   "source": [
    "Ciasteczka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd62b234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[]>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://example.com')\n",
    "print(response.cookies)  # Wyświetla ciasteczka z odpowiedzi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645afab",
   "metadata": {},
   "source": [
    "# Część 4: FastAPI i requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723532d",
   "metadata": {},
   "source": [
    "FastAPI to nowoczesny, szybki (wysokowydajny) framework do tworzenia API z Pythonem 3.7+ oparty na standardowych typach Pythona. Jest on używany do tworzenia interfejsów API*\n",
    "\n",
    "*  *https://fastapi.tiangolo.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf819b",
   "metadata": {},
   "source": [
    "Aby rozpocząć, musimy zainstalować FastAPI oraz Uvicorn, który służy jako serwer ASGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc395f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\marcin\\anaconda3\\lib\\site-packages (0.104.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: uvicorn in c:\\users\\marcin\\anaconda3\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from fastapi) (1.10.8)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from fastapi) (4.8.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272c23d",
   "metadata": {},
   "source": [
    "Tworzenie prostego endpointu w FastAPI . Oto podstawowy przykład, w którym endpoint HTTP GET zwraca słownik w formacie JSON."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb0c2803",
   "metadata": {},
   "source": [
    "# Plik main.py\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def read_root():\n",
    "    return {\"Test\": \"API\"}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efa16145",
   "metadata": {},
   "source": [
    "# Polecenie w bashu - main oznacza, że plik main.py zawiera definicję naszegj aplikacji i znajduje się w obecnym katalogu\n",
    "# --reload zapewnia restart aplikacji po zmianach w jej kodzie.\n",
    "uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e38e3a25",
   "metadata": {},
   "source": [
    "# Podgląd katalogów bazowych oraz konsola po uruchomieniu aplikacji \n",
    "Directory of C:\\Users\\Marcin\\Desktop\\fastapi\n",
    "\n",
    "09.12.2023  16:46    <DIR>          .\n",
    "09.12.2023  16:46    <DIR>          ..\n",
    "09.12.2023  16:45               115 main.py\n",
    "09.12.2023  16:46    <DIR>          __pycache__\n",
    "               1 File(s)            115 bytes\n",
    "               3 Dir(s)  19 312 545 792 bytes free\n",
    "\n",
    "(base) C:\\Users\\Marcin\\Desktop\\fastapi>uvicorn main:app --reload\n",
    "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['C:\\\\Users\\\\Marcin\\\\Desktop\\\\fastapi']\n",
    "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
    "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m15876\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
    "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8764\u001b[0m]\n",
    "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
    "\u001b[32mINFO\u001b[0m:     Application startup complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c894ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test': 'API'}\n"
     ]
    }
   ],
   "source": [
    "# Wykonujemy request do naszego API (na przykład możemy wykorzystać PyCharma) - domyślnie pod \n",
    "# http://127.0.0.1:8000 (widoczne powyżej: vicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit))\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:8000/\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbf8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jako wyjście otrzymujemy {\"Test\": \"API\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd7fb5",
   "metadata": {},
   "source": [
    "# Część 5: Zaawansowany Webscraping (dla zainteresowanych)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b120b1",
   "metadata": {},
   "source": [
    "Strony Dynamiczne:\n",
    "\n",
    "Strony dynamiczne wykorzystują JavaScript do ładowania treści asynchronicznie po załadowaniu głównej struktury strony. Oznacza to, że treści mogą być ładowane i zmieniane bez konieczności przeładowania całej strony."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9de9f",
   "metadata": {},
   "source": [
    "AJAX (Asynchronous JavaScript and XML):\n",
    "\n",
    "AJAX pozwala na wymianę danych z serwerem i aktualizację części strony bez konieczności przeładowania całej strony. Jest to kluczowy element stron dynamicznych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4950",
   "metadata": {},
   "source": [
    "Uwaga - Beautiful Soup i JavaScript! \n",
    "\n",
    "Beautiful Soup **nie jest** przystosowany do obsługi JavaScript. Potrafi analizować tylko statyczny kod HTML, który otrzymuje.\n",
    "W przypadku stron dynamicznych, które używają JavaScript do ładowania treści, Beautiful Soup nie będzie w stanie uzyskać dostępu do tych dynamicznie generowanych treści.\n",
    "W takich przypadkach lepszym rozwiązaniem jest użycie narzędzi takich jak Selenium, które potrafią obsługiwać JavaScript i dynamiczne treści."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaaecf",
   "metadata": {},
   "source": [
    "Pakiet Selenium - bardzo często wykorzystywany przy implementacji botów/crawlerów/scraperów, które pracują na stronach dynamicznych. Poniżej kilka cech zgodnie z dokumentacją:\n",
    "\n",
    "* Selenium jest narzędziem automatyzacji przeglądarek, które pozwala na interakcję ze stronami internetowymi tak, jak robiłby to prawdziwy użytkownik.\n",
    "* Selenium może uruchamiać przeglądarkę, wykonywać na niej skrypty JavaScript, kliknąć w elementy strony itp.\n",
    "* Jest to szczególnie przydatne do scrapowania stron, które silnie polegają na JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e0d176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\marcin\\anaconda3\\lib\\site-packages (4.16.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\marcin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec4e7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://example.com\")\n",
    "# Można teraz dokonywać interakcji ze stroną, np. klikając w przyciski, wypełniając formularze itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "911072a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "\n",
      "\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "# Źródło strony\n",
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3aba7",
   "metadata": {},
   "source": [
    "CAPTCHA i Bot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe5e68",
   "metadata": {},
   "source": [
    "* UWAGA: Zawsze przestrzegaj zasad i warunków korzystania ze strony!\n",
    "\n",
    "* Scrapowanie możemy rozpocząć tylko wtedy, gdy mamy zgodę właściciela treści, administratora serwera, strony lub strona zezwala na automatyzację zapytań.\n",
    "\n",
    "* Strony internetowe często używają CAPTCHA i mechanizmów wykrywania botów, aby zapobiec automatycznemu scrapowaniu i innym formom nadużyć.\n",
    "\n",
    "* Automatyczne obejście CAPTCHA i mechanizmów wykrywania botów może naruszać warunki korzystania z serwisu i być nieetyczne.\n",
    "\n",
    "* Do dużych projektów programistycznych można wykorzystać również pakiet scrapy, który z wykorzystaniem scrapy.Spider pozwala na budowę zaawansowanych scraperów."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
